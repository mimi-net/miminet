# Miminet
Эмулятор компьютерной сети для образовательных целей на базе ОС Linux.

![diagram drawio](https://github.com/mimi-net/miminet/assets/89993880/9f6ddcc2-afeb-43bd-9abf-fc34cb102e8b)<?xml version="1.0" encoding="UTF-8"?>

## AMQP
### Celery
Celery - распределенная очередь задач для асинхронной обработки сообщений. Можно написать свой инструмент для обработки сообщений при помощи библиотек kombu, aio-pika или использовать готовые фреймворки(propan, который хорошо интегрируется в асинхронные фреймворки Quart, FastAPI).

Celery выбран по следующим причинам:
1. Celery расширяет amqp, позволяя применять определенные политики в отношении задач(количество повторных попыток, временные промежутки для повторного выполнения задач, отложенные задачи, повторная попытка при определенных исключениях и т.д.).
2. Celery --- распределенная очередь сообщений, поэтому настройка многопроцессной обработки на стороне ipmininet worker'ов настраивается несколькими параметрами, без необходимости написания кода.
3. Не нужно писать логику для распределения задач на стороне worker. Вызов задач celery напоминает RPC, в заголовке amqp сообщения может отправляться имя нужного обработчика и когда celery прочитает сообщение, он вызовет нужный обработчик с нужными параметрами без необходимости настройки этого вручную.
4. Кэширование при обращении к бэкенду для оптимизации процесса отслеживания состоянии задачи.
5. Настройка стратегий для обработки отказов одного из amqp узлов(подходящая стратегия по умолчанию --- round-robin)

### Брокеры сообщений
Для решения проблемы с монолитностью приложения было необходимо выбрать брокер сообщений.
Среди рассматриваемых брокеров были Rabbitmq, основанный на протоколе AMQP и Redis. Был выбран Rabbitmq, так как он обладает продвинутой машрутизацией, надежностью доставки и множеством плагинов для равномерного распределения сообщений по очередям, а также является рекомендуемым брокером для Celery.

### Бэкенд для хранения результатов задач
Так как запросы на эмуляцию сети в Miminet кэшируются, необходимо иметь хранилище для результатов эмуляции. Для этого в текущей реализации использовалась СУБД sqlite. Для хранения результатов также используется временное хранилище, из которого Application Server забирает результаты и сохраняет в постоянное хранилище(для хранения результатов без промежуточного звена пришлось бы сильнее менять архитектуру приложения и используемую СУБД). 

### Хранение результатов в Rabbitmq
Для хранения результатов можно использовать очереди Rabbitmq, которые будут создавать клиенты. Однако, такой подход обладает множеством недостатков, связанных с созданием и управлением этими очередями, что является проблемой, особенно в случае недолгоживущего клиента. Для решения этой проблемы можно использовать механизм direct reply-to, однако он имеет определенные ограничения и решение все равно останется недостаточно производительным.

### Хранение результатов в Redis
Redis является более производительным решением и имеет более продвинутые параметры для настройки хранения и отслеживания результатов задач(лучше интегрирован как бэкенд для Celery).

## Развертывание ipmininet worker
Для своей работы mininet требует root привелегии для работы с routing tables хоста, настройкам интерфейса и т.д., в связи с чем для безопасной работы было решено использовать виртуальные машины.

### Требования:
1. Быстрота развертывания (можно развернуть очередной worker на новом хосте парой команд)
2. Переносимость(в случае чего, есть возможность развернуть эту систему не только под Ubuntu и Fedora).

### Vagrant
Vagrant --- менеджер виртуальных машин для создания и управлениями средами разработки. На сервере необходимо наличие vagrant и виртуальной машины. Всю работу(переброска портов, синхронизация shared folder и скачивание зависимостей) выполнит Vagrant при помощи ansible. В настоящее время настроена работа для virtualbox и vmware. Несмотря на то что ipmininet поддерживает Vagrant, ipmininet box занимает в три раза больше места, а мы хотим избавиться от лишних зависимостей.

### Процесс развертывания worker на данный момент.
Разрешение vagrant для NFS(для полной автоматизации vagrant up):
```
# /etc/sudoers.d/vagrant-syncedfolders
Cmnd_Alias VAGRANT_EXPORTS_CHOWN = /bin/chown 0\:0 /tmp/vagrant-exports
Cmnd_Alias VAGRANT_EXPORTS_MV = /bin/mv -f /tmp/vagrant-exports /etc/exports
Cmnd_Alias VAGRANT_NFSD_CHECK = /etc/init.d/nfs-kernel-server status
Cmnd_Alias VAGRANT_NFSD_START = /etc/init.d/nfs-kernel-server start
Cmnd_Alias VAGRANT_NFSD_APPLY = /usr/sbin/exportfs -ar
%sudo ALL=(root) NOPASSWD: VAGRANT_EXPORTS_CHOWN, VAGRANT_EXPORTS_MV, VAGRANT_NFSD_CHECK, VAGRANT_NFSD_START, VAGRANT_NFSD_APPLY
```

```
export numberOfBoxes=N
export provider=vbox/vmware
. vagrant_vms.sh
```
N - количество экземпляров vagrant(в целом бесполезная вещь, но так как Miminet на данный момент не поддерживает мультипроцессинг, выходом является запуск нескольких вм).

Подразумевается что на хосте есть .env(или .env.yml) файл следущего содержания:
```
amqp_urls=amqp://user:password@host:port/,amqp://user:password@host:port/
backend_urls= redis://:password@host:port/db
celery_concurrency=N
queue_names=queue1,queue2,queue3
exchange_name=default_exchange
```
Очевидно, что эту часть можно автоматизировать при помощи ansible и передавать все эти параметры хосту, но пока что это не написано.

### Local ipmininet deployment.
Также для удобства локального разветрывания написан docker compose.
```
cd back && docker compose up -d
``` 
### Отправляемые url
Обратите внимание, что в зависимости от использования vagrant/docker, вам придется указать разные amqp url.
Например, если вы используете docker compose для ipmininet worker и rabbitmq, вам необходимо указать external network в docker-compose.ym. Для одноразового использования(до перезагрузки контейнера):
```
docker network create RabbitmqNetwork
docker network connect RabbitmqNetwork celery-container-name
docker network connect RabbitmqNetwork rabbitmq-container-name
```
Тогда amqp url будет выглядеть как amqp://user:password@rabbitmq-container-name/

Если же вы развернули worker с помощью vagrant в guest, а rabbitmq на хосте, то amqp url (192.168.56.1 по умолчанию ip хоста для private network с virtualbox) будет выглядеть как amqp://user:password@192.168.56.1:port/
